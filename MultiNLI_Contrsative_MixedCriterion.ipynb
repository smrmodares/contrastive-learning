{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFdrCvPQe9ie"
   },
   "source": [
    "# Prerequisities\n",
    "\n",
    "First, we install and import libraries we'll need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SabUS2W8v2fF"
   },
   "outputs": [],
   "source": [
    "# %pip install datasets\n",
    "# %pip install transformers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertModel, BertTokenizer\n",
    "# from transformers import AutoTokenizer, GPT2Config, PreTrainedModel\n",
    "# from transformers import AutoModelForCausalLM, PretrainedConfig, AutoConfig, AutoModel, AutoImageProcessor\n",
    "\n",
    "# import torchtext.datasets as datasets\n",
    "\n",
    "import os, math\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUOqfrV-73JA"
   },
   "source": [
    "Next, we'll set the random seeds for reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I8P_0RU64f1w"
   },
   "outputs": [],
   "source": [
    "SEED = 43\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpQcbJzlDeI0"
   },
   "source": [
    "Setting the device option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zxYj7q3YDjRX"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { \n",
    "    'LLM_BATCH_SIZE': 50,\n",
    "    'LLM_EPOCHS': 1,\n",
    "    'LLM_loss>0': True,\n",
    "    'temperature': 1,\n",
    "    'loss_function': 'sina_loss()',\n",
    "    'classifier_lr': 5e-1,\n",
    "    'classifier_batch': 50,\n",
    "    'classifier_epochs': 3,\n",
    "    'base_model_optimizer': 'optim.Adam(base_model.parameters(), lr=1e-5)',\n",
    "    'classifier_optimizer': 'optim.Adam(model.parameters(), lr=1e-5)',\n",
    "    'use_scheduler': False,\n",
    "    'scheduler': 'optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.5)',\n",
    "    'base_model_train_loss': 0,\n",
    "    'base_model_valid_loss': 0,\n",
    "    'loss_train': 0,\n",
    "    'loss_valid': 0,\n",
    "    'acc_train': 0,\n",
    "    'acc_valid': 0,\n",
    "    'precision_test': 0,\n",
    "    'recall_test': 0,\n",
    "    'f1_test': 0,\n",
    "    'acc_test': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VaONGukRfHh"
   },
   "source": [
    "# The Data\n",
    "\n",
    "In this exercise we are going to use MultiNLI Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aJcjnHX9NFBA"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"multi_nli\")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysj9L10Bnt8u"
   },
   "source": [
    "## Data fields\n",
    "\n",
    "The data fields are the same among all splits.\n",
    "\n",
    "*   *promptID:* Unique identifier for prompt\n",
    "\n",
    "*   *pairID:* Unique identifier for pair\n",
    "*   *{premise,hypothesis}:* combination of premise and hypothesis\n",
    "*   *{premise,hypothesis} parse:* Each sentence as parsed by the Stanford PCFG Parser 3.5.2\n",
    "*   *{premise,hypothesis} binary parse:* parses in unlabeled binary-branching format\n",
    "*   *genre:* a string feature.\n",
    "*   *label:* a classification label, with possible values including entailment (0), neutral (1), contradiction (2). Dataset instances which don't have any gold label are marked with -1 label. Make sure you filter them before starting the training using datasets.Dataset.filter.\n",
    "\n",
    "You can see the number of rows in each split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lu7I7t7eDSWV",
    "outputId": "ee6edef0-04db-4e6f-a2d6-580b753b8c6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XKbIcz_VBUkB",
    "outputId": "14d15d85-f8ee-43c5-e5be-f89c7455f62f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyvWqAtgzQCh"
   },
   "source": [
    "Defining some key variables that will be used later on in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "86lCs2tIzMAz"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 120\n",
    "LR = 1e-05\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', truncation=True, do_lower_case=True)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', truncation=True, do_lower_case=True)\n",
    "MAX_LEN = 320 #tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "82vk5ed20SkP"
   },
   "outputs": [],
   "source": [
    "class MNLIDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['premise'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        premise = str(self.data['premise'][index])\n",
    "        premise = \" \".join(premise.split())\n",
    "\n",
    "        hypothesis = str(self.data['hypothesis'][index])\n",
    "        hypothesis = \" \".join(hypothesis.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            premise,\n",
    "            hypothesis,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': ids.squeeze(),\n",
    "            'mask': mask.squeeze(),\n",
    "            'token_type_ids': token_type_ids.squeeze(),\n",
    "            'targets': torch.tensor(self.data['label'][index], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CAjPOoyp5421"
   },
   "outputs": [],
   "source": [
    "# Use slicing to pick as much as data you want. EX: dataset['train'][:3000].\n",
    "train_dataset = MNLIDataset(dataset['train'][:100000], tokenizer, MAX_LEN)\n",
    "valid_dataset = MNLIDataset(dataset['train'][100000:110000], tokenizer, MAX_LEN)\n",
    "test_dataset = MNLIDataset(dataset['validation_matched'], tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mag9od5ddiRK",
    "outputId": "61b3aff9-8020-4715-ab47-69e37efbf387"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-Q_eKT5Gaw3"
   },
   "source": [
    "Here we define our dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_DMZBMmDmaR7"
   },
   "outputs": [],
   "source": [
    "train_dataloader = data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, drop_last=False)\n",
    "valid_dataloader = data.DataLoader(valid_dataset, shuffle=True, batch_size=BATCH_SIZE, drop_last=False)\n",
    "test_dataloader = data.DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Objective\n",
    "\n",
    "Now we define our contrastive loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sina_loss():\n",
    "    \n",
    "    def forward(self, logits, labels):\n",
    "        # t = 0.07\n",
    "        t = 35\n",
    "        loss = 0\n",
    "        # mean_logits = torch.mean(logits[:, 1:-1, :], axis=1)\n",
    "        cls_logits = logits[:, 0]\n",
    "\n",
    "        for i, logit in enumerate(cls_logits):\n",
    "            \n",
    "            positives = []\n",
    "            negatives = []\n",
    "            for j in range(len(labels)):\n",
    "                if labels[i] != labels[j]:\n",
    "                    negatives.append(cls_logits[j].tolist())\n",
    "                elif i != j:\n",
    "                    positives.append(cls_logits[j].tolist())\n",
    "                    \n",
    "            if len(positives) != 0 and len(negatives) != 0:\n",
    "                positives = torch.tensor(positives).to(device)\n",
    "                pos_sum = torch.sum(torch.exp((positives @ logit) / t))\n",
    "                negatives = torch.tensor(negatives).to(device)\n",
    "                neg_sum = torch.sum(torch.exp((negatives @ logit) / t))\n",
    "#                 loss += torch.log(neg_sum/pos_sum + neg_sum)\n",
    "                loss += -torch.log(pos_sum/neg_sum)\n",
    "\n",
    "        return loss if loss != 0 else torch.tensor(0.0, dtype=torch.float32, device='cuda:0', requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "base_model.to(device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Base Model Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the loss function and optimizer\n",
    "base_model_optimizer = eval(config['base_model_optimizer'])\n",
    "\n",
    "base_model_criterion = eval(config['loss_function'])\n",
    "base_model_criterion = base_model_criterion#.to(device)\n",
    "\n",
    "base_model_criterion2 = nn.CrossEntropyLoss()\n",
    "base_model_criterion2 = base_model_criterion2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_base_model(model, optimizer, data_loader, dataset, criterion, criterion2):\n",
    "#     # Set model to train mode\n",
    "#     model.train()\n",
    "#     # loss per epoch and number of correct predictions in order to calculate the accuracy\n",
    "#     epoch_loss = 0\n",
    "#     n_correct = 0\n",
    "\n",
    "#     for idx, data in tqdm(enumerate(data_loader), desc='Training', leave=False):\n",
    "\n",
    "#         ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
    "#         ids = data['ids'].to(device, dtype = torch.long)\n",
    "#         mask = data['mask'].to(device, dtype = torch.long)\n",
    "#         token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "#         targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "#         ## Step 2: Run the model on the input data        \n",
    "#         output = model(input_ids=ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "#         last_hidden_state = output[0] # last_hidden_state (batch_size, sequence_length, hidden_size)\n",
    "#         # pooler = last_hidden_state[:, 0] # take only the CLS token from last_hidden_state\n",
    "        \n",
    "#         ## Step 3: Calculate the loss and accuracy\n",
    "#         if idx % 2 == 0:\n",
    "#             loss = criterion.forward(last_hidden_state, targets)\n",
    "#         else:\n",
    "#             loss = criterion2()\n",
    "\n",
    "#         ## Step 4: Perform backpropagation\n",
    "#         # Before calculating the gradients, we need to ensure that they are all zero.\n",
    "#         # The gradients would not be overwritten, but actually added to the existing ones.\n",
    "#         optimizer.zero_grad()\n",
    "#         # Perform backpropagation\n",
    "#         loss.backward()\n",
    "\n",
    "#         ## Step 5: Update the parameters\n",
    "#         optimizer.step()\n",
    "\n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#     return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_base_model(model, data_loader, dataset, criterion):\n",
    "#     # Set model to eval mode\n",
    "#     model.eval()\n",
    "#     # loss per epoch\n",
    "#     epoch_loss = 0\n",
    "\n",
    "#     with torch.no_grad(): # Deactivate gradients for the following code\n",
    "#         for data in tqdm(data_loader, desc='Evaluation', leave=False):\n",
    "\n",
    "#             ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
    "#             ids = data['ids'].to(device, dtype = torch.long)\n",
    "#             mask = data['mask'].to(device, dtype = torch.long)\n",
    "#             token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "#             targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "#             ## Step 2: Run the model on the input data\n",
    "#             output = model(input_ids=ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "#             last_hidden_state = output[0] # last_hidden_state (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "#             ## Step 3: Calculate the loss and accuracy\n",
    "#             loss = criterion.forward(last_hidden_state, targets)\n",
    "            \n",
    "#             epoch_loss += loss.item()\n",
    "\n",
    "#     return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1ab92ce71f42cdbac8e0fba1c2c98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain loss: 169.082\n",
      "\tValid loss: 175.799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02\n",
      "\tTrain loss: 163.949\n",
      "\tValid loss: 185.103\n"
     ]
    }
   ],
   "source": [
    "# EPOCHS = 3\n",
    "\n",
    "# best_valid_loss_base_model = float('inf')\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in tqdm(range(EPOCHS), desc='Epochs'):\n",
    "\n",
    "#     train_loss_base_model = train_base_model(base_model, \n",
    "#                                              base_model_optimizer,\n",
    "#                                              train_dataloader,\n",
    "#                                              train_dataset, \n",
    "#                                              base_model_criterion,\n",
    "#                                              base_model_criterion2)\n",
    "    \n",
    "#     valid_loss_base_model = eval_base_model(base_model, \n",
    "#                                             valid_dataloader,\n",
    "#                                             valid_dataset, \n",
    "#                                             base_model_criterion)\n",
    "\n",
    "#     if valid_loss_base_model < best_valid_loss_base_model:\n",
    "#         best_valid_loss_base_model = valid_loss_base_model\n",
    "#         torch.save(base_model.state_dict(), 'MNLI_bert_base_uncased_sina_loss_checkpoint3.pt')\n",
    "\n",
    "#     print(f'Epoch: {epoch + 1:02}')\n",
    "#     print(f'\\tTrain loss: {train_loss_base_model:.3f}')\n",
    "#     print(f'\\tValid loss: {valid_loss_base_model:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNLI_bert_base_uncased_sina_loss_checkpoint2.pt\n",
    "\n",
    "Epoch: 02\n",
    "\tTrain loss: 173.986\n",
    "\tValid loss: 174.286\n",
    "    \n",
    "    \n",
    "cosine similarity\n",
    "\n",
    "merge loss\n",
    "\n",
    "new language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base_model.load_state_dict(torch.load('./MNLI_bert_base_uncased_sina_loss_checkpoint2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config['base_model_train_loss'] = round(train_loss_base_model, 2)\n",
    "# config['base_model_valid_loss'] = round(valid_loss_base_model, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XUDcKOO_AQD"
   },
   "source": [
    "# The Model\n",
    "\n",
    "The model we are going to use is BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "W8W19LUQAZ42"
   },
   "outputs": [],
   "source": [
    "# Defining BERT model and adding classfication head on top of it\n",
    "class BertClass(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(BertClass, self).__init__()\n",
    "#         self.bare_bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "#         self.bare_bert = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "        self.bare_bert = base_model\n",
    "        self.dense = nn.Linear(768, 768)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(768, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "\n",
    "        output_1 = self.bare_bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)\n",
    "        hidden_state = output_1[0] # last_hidden_state (batch_size, sequence_length, hidden_size)\n",
    "        pooler = hidden_state[:, 0] # take only the CLS token from last_hidden_state\n",
    "        # pooler = hidden_state[:, 1:].mean(1)\n",
    "        pooler = self.dense(pooler)\n",
    "        pooler = nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output, output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "AzmPesIySmVT"
   },
   "outputs": [],
   "source": [
    "model = BertClass(base_model)\n",
    "model.to(device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEEhM9zzVNKu"
   },
   "source": [
    "# Fine-tuning\n",
    "\n",
    "Here we fine-tune the generated model from previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "N7WyQXiPVSOR"
   },
   "outputs": [],
   "source": [
    "# Defining the loss function and optimizer\n",
    "optimizer = eval(config['classifier_optimizer']) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PLCl1Vh8VhW8"
   },
   "outputs": [],
   "source": [
    "# Accuracy function\n",
    "def calculate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "rNDYDGw-V1a1"
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, data_loader, dataset, criterion):\n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "    # loss per epoch and number of correct predictions in order to calculate the accuracy\n",
    "    epoch_loss = 0\n",
    "    n_correct = 0\n",
    "    \n",
    "    for idx, data in enumerate(tqdm(data_loader, desc='Training', leave=False)):\n",
    "\n",
    "        ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        ## Step 2: Run the model on the input data\n",
    "        preds, bert_rep = model(ids, mask, token_type_ids)\n",
    "        last_hidden_state = bert_rep[0]\n",
    "        \n",
    "        ## Step 3: Calculate the loss and accuracy\n",
    "        if idx % 2 == 0:\n",
    "            loss = criterion(preds, targets)\n",
    "        else:\n",
    "            loss = criterion.forward(last_hidden_state, targets)\n",
    "\n",
    "        max_val, max_idx = torch.max(preds.data, dim=1)\n",
    "        n_correct += calculate_accuracy(max_idx, targets)\n",
    "\n",
    "        ## Step 4: Perform backpropagation\n",
    "        # Before calculating the gradients, we need to ensure that they are all zero.\n",
    "        # The gradients would not be overwritten, but actually added to the existing ones.\n",
    "        optimizer.zero_grad()\n",
    "        # Perform backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        ## Step 5: Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(data_loader), (n_correct / len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "OqVRVuh5Ar3D"
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, dataset, criterion):\n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # loss per epoch and number of correct predictions in order to calculate the accuracy\n",
    "    epoch_loss = 0\n",
    "    n_correct = 0\n",
    "    preds_list = list()\n",
    "    targets_list = list()\n",
    "\n",
    "    with torch.no_grad(): # Deactivate gradients for the following code\n",
    "        for data in tqdm(data_loader, desc='Evaluation', leave=False):\n",
    "\n",
    "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            # print(targets.tolist())\n",
    "            targets_list.extend(targets.tolist())\n",
    "\n",
    "            ## Step 2: Run the model on the input data\n",
    "            preds, _ = model(ids, mask, token_type_ids)\n",
    "            preds = preds.squeeze()\n",
    "            # print(preds.tolist())\n",
    "            preds_list.extend(preds.tolist())\n",
    "\n",
    "            ## Step 3: Calculate the loss and accuracy\n",
    "            loss = criterion(preds, targets)\n",
    "\n",
    "            max_val, max_idx = torch.max(preds.data, dim=1)\n",
    "            n_correct += calculate_accuracy(max_idx, targets)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(data_loader), (n_correct / len(dataset)), preds_list, targets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538,
     "referenced_widgets": [
      "bbe0ff8d3f704f9c9b7b08c357a6a413",
      "c50647266db342e18fff5759e207a190",
      "a1f88e04742e41da975ec55a15ba79e9",
      "1cbc52687f064eb4bfe8ac8ed954955f",
      "2a18747d4f8144f08f0fe1af2388cb7b",
      "1bc4077eb74e44a4b0709f6b1edd41c2",
      "50f031e035244b54a28cea7dc6c814dd",
      "5a0d291f3ca644ac810e0f1541767f91",
      "17c71ec216c54aa5b29943822fd066a3",
      "f2ef178def7d4d9f86f3e0be5555233c",
      "098eed04ea754db5bdeb0239b08f5c5c",
      "289438bbbb9f4a2d930a428eaf2cee24",
      "3f064f9177284cbb968cf9f7539bf530",
      "a5d9a2fc3e9f46b09429d0e80724f0a4",
      "6515568e39404cd0b06525c2c91dd5e3",
      "fffe0bfaa09846c185a8eff3ffcd6627",
      "25852f88a5f54739a7df243fc4824121",
      "b03afaa6b50b43318528fafcb7bd8b93",
      "d45adffa540d4ad7b1d8ea20a6b1ed5c",
      "8b110dab10b54b368da9bdd83f339467",
      "692124cbd39e4fc6ae0c39d8c6dff43c",
      "901e5bca3da84dd7a084c540bb7e0ad8"
     ]
    },
    "id": "NYCgRsvuthYE",
    "outputId": "db2ebc02-bd09-48a4-fa14-29205b691c75"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0551d5294208404198bc0c8ab550f2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c990629ab74c4ea1feaad37d15233e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 6\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(EPOCHS), desc='Epochs'):\n",
    "\n",
    "    train_loss, train_acc = train_model(model, optimizer, train_dataloader, train_dataset, criterion)\n",
    "    valid_loss, valid_acc, _, _ = eval_model(model, valid_dataloader, valid_dataset, criterion)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'MNLI_bert_base_uncased_classifier_sina_loss_checkpoint2.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02}')\n",
    "    print(f'\\tTrain loss: {train_loss:.3f} | Train acc: {train_acc * 100:.2f}%')\n",
    "    print(f'\\tValid loss: {valid_loss:.3f} | Valid acc: {valid_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['loss_train'] = round(train_loss, 2)\n",
    "config['loss_valid'] = round(valid_loss, 2)\n",
    "config['acc_train'] = round(train_acc, 2)\n",
    "config['acc_valid'] = round(valid_acc, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEWRxesQ1U9O"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "Now, we evaluate the performance of the fine-tuned model on the test set by computing metrics like accuracy, precision, recall, and F1-score. First we load the best checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwrbqS-z7hmx",
    "outputId": "44519ad4-bca8-4a7a-b61e-6ce32c1f060b"
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/content/drive/MyDrive/NLP 01 - Dr. Pilehvar/checkpoint_Rotten_RoBERTa.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiV2rYYW2dPX"
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35,
     "referenced_widgets": [
      "b814fca824f24199941aacdef66fd9e5",
      "713c3502649f41cf8345d107b1161532",
      "23bd649965fc49f4a5921e2cd33833a2",
      "5e8d1fd758ec47b89c4a05e3209c1bb2",
      "9c53d0353d9341da974eeb05d5997fe7",
      "fa96b2fbe6ea4002bb136986bb121d91",
      "d8a4f651b0324932a2bad8e57a0225a7",
      "1ce752ca6df34865a43b305c1b303cbd",
      "ce510d79e46a413b83c99f129d416664",
      "3b8c815a9bfe48d6a142da8482a81e59",
      "b804ba681dc34732be760267484009ac"
     ]
    },
    "id": "yVB9FaTTHRuS",
    "outputId": "f75a123e-5622-4ee1-c7f4-1cac7e72edff"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.610 | Test acc: 80.43%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, preds_list, targets_list = eval_model(model, test_dataloader, test_dataset, criterion)\n",
    "print(f'Test loss: {test_loss:.3f} | Test acc: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEtuqNEaOPLe"
   },
   "source": [
    "Using torchmetrics to calculate remaining metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1U0mnCuoRPXX"
   },
   "outputs": [],
   "source": [
    "# %pip install torchmetrics\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "p5oT-efdtdYn"
   },
   "outputs": [],
   "source": [
    "from torchmetrics.classification import Precision, Recall, F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Aj8qcYZUPMd",
    "outputId": "ca4020d7-00db-47f7-a960-aff07e1fe4a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9815\n",
      "9815\n"
     ]
    }
   ],
   "source": [
    "# generated prediction and target lists are correct in size\n",
    "print(len(preds_list))\n",
    "print(len(targets_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "LkZ_Z0yfU8No"
   },
   "outputs": [],
   "source": [
    "# torchmetrics uses tensors\n",
    "preds_tensor = torch.tensor(preds_list)\n",
    "targets_tensor = torch.tensor(targets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "njdgp4XrVnMW"
   },
   "outputs": [],
   "source": [
    "# max for each prediction\n",
    "preds_tensor_max = torch.max(preds_tensor, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 80.38%\n",
      "Recall: 80.35%\n",
      "F-1 Score: 80.35%\n"
     ]
    }
   ],
   "source": [
    "## Precision\n",
    "\n",
    "precision_metric = Precision(task=\"multiclass\", average='macro', num_classes=3)\n",
    "precision = precision_metric(preds_tensor_max.indices, targets_tensor).item()\n",
    "print(f'Precision: {precision * 100:.2f}%')\n",
    "\n",
    "## Recall\n",
    "\n",
    "recall_metric = Recall(task=\"multiclass\", average='macro', num_classes=3)\n",
    "recall = recall_metric(preds_tensor_max.indices, targets_tensor).item()\n",
    "print(f'Recall: {recall * 100:.2f}%')\n",
    "\n",
    "## F-1 Score\n",
    "\n",
    "f1score_metric = F1Score(task=\"multiclass\", average='macro', num_classes=3)\n",
    "f1score = f1score_metric(preds_tensor_max.indices, targets_tensor)\n",
    "print(f'F-1 Score: {f1score * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['precision_test'] = round(precision, 4)\n",
    "config['recall_test'] = round(recall, 4)\n",
    "config['f1_test'] = round(f1score.item(), 4)\n",
    "config['acc_test'] = round(test_acc, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename = 'dict_results.json'\n",
    "# 1. Read file contents\n",
    "with open(filename, \"r\") as file:\n",
    "    data_json = json.load(file)\n",
    "# 2. Update json object\n",
    "data_json.append(config)\n",
    "# 3. Write json file\n",
    "with open(filename, \"w\") as file:\n",
    "    json.dump(data_json, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VR8fZRUcBx7E",
    "outputId": "5b8918aa-2af3-4810-e51e-8aa3bf5dc955"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token_id\n",
    "tokenizer.cls_token_id\n",
    "tokenizer.pad_token_id\n",
    "tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "dW48i1UqE7SC",
    "outputId": "87da9dce-1ac9-41f9-c6dc-a74e86315fa0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[UNK]'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token\n",
    "tokenizer.cls_token\n",
    "tokenizer.pad_token\n",
    "tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "HmnCC-QcGIMn"
   },
   "outputs": [],
   "source": [
    "# inputs = tokenizer.encode_plus(\n",
    "#     'Hi my name is sina.',\n",
    "#     'Where are you?',\n",
    "#     add_special_tokens=True,\n",
    "#     max_length=MAX_LEN,\n",
    "#     return_tensors= 'pt',\n",
    "#     padding='max_length',\n",
    "#     return_token_type_ids=True,\n",
    "#     truncation=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "VS2nelw2Q2l2"
   },
   "outputs": [],
   "source": [
    "# inputs['input_ids'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "-jqNEtPQHX3c"
   },
   "outputs": [],
   "source": [
    "# tokenizer.decode(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hi5qWcJLQnFM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "098eed04ea754db5bdeb0239b08f5c5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "17c71ec216c54aa5b29943822fd066a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1bc4077eb74e44a4b0709f6b1edd41c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1cbc52687f064eb4bfe8ac8ed954955f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2ef178def7d4d9f86f3e0be5555233c",
      "placeholder": "​",
      "style": "IPY_MODEL_098eed04ea754db5bdeb0239b08f5c5c",
      "value": " 0/6 [01:13&lt;?, ?it/s]"
     }
    },
    "25852f88a5f54739a7df243fc4824121": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "289438bbbb9f4a2d930a428eaf2cee24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3f064f9177284cbb968cf9f7539bf530",
       "IPY_MODEL_a5d9a2fc3e9f46b09429d0e80724f0a4",
       "IPY_MODEL_6515568e39404cd0b06525c2c91dd5e3"
      ],
      "layout": "IPY_MODEL_fffe0bfaa09846c185a8eff3ffcd6627"
     }
    },
    "2a18747d4f8144f08f0fe1af2388cb7b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f064f9177284cbb968cf9f7539bf530": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25852f88a5f54739a7df243fc4824121",
      "placeholder": "​",
      "style": "IPY_MODEL_b03afaa6b50b43318528fafcb7bd8b93",
      "value": "Training:   2%"
     }
    },
    "50f031e035244b54a28cea7dc6c814dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a0d291f3ca644ac810e0f1541767f91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6515568e39404cd0b06525c2c91dd5e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_692124cbd39e4fc6ae0c39d8c6dff43c",
      "placeholder": "​",
      "style": "IPY_MODEL_901e5bca3da84dd7a084c540bb7e0ad8",
      "value": " 63/3125 [01:13&lt;54:33,  1.07s/it]"
     }
    },
    "692124cbd39e4fc6ae0c39d8c6dff43c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b110dab10b54b368da9bdd83f339467": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "901e5bca3da84dd7a084c540bb7e0ad8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1f88e04742e41da975ec55a15ba79e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a0d291f3ca644ac810e0f1541767f91",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_17c71ec216c54aa5b29943822fd066a3",
      "value": 0
     }
    },
    "a5d9a2fc3e9f46b09429d0e80724f0a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d45adffa540d4ad7b1d8ea20a6b1ed5c",
      "max": 3125,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b110dab10b54b368da9bdd83f339467",
      "value": 63
     }
    },
    "b03afaa6b50b43318528fafcb7bd8b93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bbe0ff8d3f704f9c9b7b08c357a6a413": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c50647266db342e18fff5759e207a190",
       "IPY_MODEL_a1f88e04742e41da975ec55a15ba79e9",
       "IPY_MODEL_1cbc52687f064eb4bfe8ac8ed954955f"
      ],
      "layout": "IPY_MODEL_2a18747d4f8144f08f0fe1af2388cb7b"
     }
    },
    "c50647266db342e18fff5759e207a190": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1bc4077eb74e44a4b0709f6b1edd41c2",
      "placeholder": "​",
      "style": "IPY_MODEL_50f031e035244b54a28cea7dc6c814dd",
      "value": "Epochs:   0%"
     }
    },
    "d45adffa540d4ad7b1d8ea20a6b1ed5c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2ef178def7d4d9f86f3e0be5555233c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fffe0bfaa09846c185a8eff3ffcd6627": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
